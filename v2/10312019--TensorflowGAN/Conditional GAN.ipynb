{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset parameters.\n",
    "num_features = 784 # data features (img shape: 28*28).\n",
    "\n",
    "# Training parameters.\n",
    "lr_generator = 0.0002\n",
    "lr_discriminator = 0.0002\n",
    "training_steps = 200000\n",
    "batch_size = 128\n",
    "display_step = 500\n",
    "\n",
    "# Network parameters.\n",
    "noise_dim = 100 # Noise data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST data.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Model.\n",
    "class Generator(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = layers.Dense(7 * 7 * 128)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2tr1 = layers.Conv2DTranspose(64, 5, strides=2, padding='SAME')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv2tr2 = layers.Conv2DTranspose(1, 5, strides=2, padding='SAME')\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, label, is_training=False):\n",
    "        x = tf.concat([x, label],axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        # Reshape to a 4-D array of images: (batch, height, width, channels)\n",
    "        # New shape: (batch, 7, 7, 128)\n",
    "        x = tf.reshape(x, shape=[-1, 7, 7, 128])\n",
    "        # Deconvolution, image shape: (batch, 14, 14, 64)\n",
    "        x = self.conv2tr1(x)\n",
    "        x = self.bn2(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        # Deconvolution, image shape: (batch, 28, 28, 1)\n",
    "        x = self.conv2tr2(x)\n",
    "        x = tf.nn.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Generator Network\n",
    "# Input: Noise, Output: Image\n",
    "# Note that batch normalization has different behavior at training and inference time,\n",
    "# we then use a placeholder to indicates the layer if we are training or not.\n",
    "class Discriminator(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(64, 5, strides=2, padding='SAME')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(128, 5, strides=2, padding='SAME')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(1024)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.fc2 = layers.Dense(2)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, label, is_training=False):\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = tf.concat([x, label],axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Build neural network model.\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses.\n",
    "def generator_loss(reconstructed_image):\n",
    "    gen_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=reconstructed_image, labels=tf.ones([batch_size], dtype=tf.int32)))\n",
    "    return gen_loss\n",
    "\n",
    "def discriminator_loss(disc_fake, disc_real):\n",
    "    disc_loss_real = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=disc_real, labels=tf.ones([batch_size], dtype=tf.int32)))\n",
    "    disc_loss_fake = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=disc_fake, labels=tf.zeros([batch_size], dtype=tf.int32)))\n",
    "    return disc_loss_real + disc_loss_fake\n",
    "\n",
    "# Optimizers.\n",
    "optimizer_gen = tf.optimizers.Adam(learning_rate=lr_generator)\n",
    "optimizer_disc = tf.optimizers.Adam(learning_rate=lr_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. Inputs: real image and noise.\n",
    "def run_optimization(real_images, labels):\n",
    "    \n",
    "    # Rescale to [-1, 1], the input range of the discriminator\n",
    "    real_images = real_images * 2. - 1.\n",
    "\n",
    "    # Generate noise.\n",
    "    noise = np.random.normal(-1., 1., size=[batch_size, noise_dim]).astype(np.float32)\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "            \n",
    "        fake_images = generator(noise, labels, is_training=True)\n",
    "        disc_fake = discriminator(fake_images, labels, is_training=True)\n",
    "        disc_real = discriminator(real_images, labels, is_training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(disc_fake, disc_real)\n",
    "            \n",
    "    # Training Variables for each optimizer\n",
    "    gradients_disc = g.gradient(disc_loss,  discriminator.trainable_variables)\n",
    "    optimizer_disc.apply_gradients(zip(gradients_disc,  discriminator.trainable_variables))\n",
    "    \n",
    "    # Generate noise.\n",
    "    noise = np.random.normal(-1., 1., size=[batch_size, noise_dim]).astype(np.float32)\n",
    "    \n",
    "    with tf.GradientTape() as g:\n",
    "            \n",
    "        fake_images = generator(noise, labels, is_training=True)\n",
    "        disc_fake = discriminator(fake_images, labels, is_training=True)\n",
    "\n",
    "        gen_loss = generator_loss(disc_fake)\n",
    "            \n",
    "    gradients_gen = g.gradient(gen_loss, generator.trainable_variables)\n",
    "    optimizer_gen.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial: gen_loss: 0.687370, disc_loss: 1.442660\n",
      "step: 500, gen_loss: 2.375842, disc_loss: 0.290571\n",
      "step: 1000, gen_loss: 2.128632, disc_loss: 0.440344\n",
      "step: 1500, gen_loss: 2.849523, disc_loss: 0.358814\n",
      "step: 2000, gen_loss: 2.634591, disc_loss: 0.268755\n",
      "step: 2500, gen_loss: 3.196488, disc_loss: 0.245166\n",
      "step: 3000, gen_loss: 3.741001, disc_loss: 0.171512\n",
      "step: 3500, gen_loss: 3.587307, disc_loss: 0.142541\n",
      "step: 4000, gen_loss: 3.607323, disc_loss: 0.124568\n",
      "step: 4500, gen_loss: 4.302686, disc_loss: 0.204791\n",
      "step: 5000, gen_loss: 3.025012, disc_loss: 0.273140\n",
      "step: 5500, gen_loss: 3.206779, disc_loss: 0.251626\n",
      "step: 6000, gen_loss: 2.930162, disc_loss: 0.356553\n",
      "step: 6500, gen_loss: 2.653489, disc_loss: 0.505986\n",
      "step: 7000, gen_loss: 2.525524, disc_loss: 0.629481\n",
      "step: 7500, gen_loss: 2.169511, disc_loss: 0.514819\n",
      "step: 8000, gen_loss: 2.370711, disc_loss: 0.570225\n",
      "step: 8500, gen_loss: 3.063212, disc_loss: 0.341553\n",
      "step: 9000, gen_loss: 2.587950, disc_loss: 0.498329\n",
      "step: 9500, gen_loss: 3.094188, disc_loss: 0.317661\n",
      "step: 10000, gen_loss: 2.954916, disc_loss: 0.346519\n",
      "step: 10500, gen_loss: 2.493813, disc_loss: 0.332238\n",
      "step: 11000, gen_loss: 2.779856, disc_loss: 0.295779\n",
      "step: 11500, gen_loss: 3.513341, disc_loss: 0.338437\n",
      "step: 12000, gen_loss: 3.283134, disc_loss: 0.371151\n",
      "step: 12500, gen_loss: 2.627971, disc_loss: 0.247383\n",
      "step: 13000, gen_loss: 3.092351, disc_loss: 0.275342\n",
      "step: 13500, gen_loss: 3.116591, disc_loss: 0.337058\n",
      "step: 14000, gen_loss: 3.160445, disc_loss: 0.378065\n",
      "step: 14500, gen_loss: 3.422069, disc_loss: 0.342220\n",
      "step: 15000, gen_loss: 3.301665, disc_loss: 0.332652\n",
      "step: 15500, gen_loss: 4.051611, disc_loss: 0.259162\n",
      "step: 16000, gen_loss: 3.012552, disc_loss: 0.187308\n",
      "step: 16500, gen_loss: 3.928785, disc_loss: 0.390405\n",
      "step: 17000, gen_loss: 3.754734, disc_loss: 0.133976\n",
      "step: 17500, gen_loss: 3.838304, disc_loss: 0.204271\n",
      "step: 18000, gen_loss: 4.429111, disc_loss: 0.291195\n",
      "step: 18500, gen_loss: 3.615666, disc_loss: 0.197616\n",
      "step: 19000, gen_loss: 3.956524, disc_loss: 0.432047\n",
      "step: 19500, gen_loss: 5.301174, disc_loss: 0.375170\n",
      "step: 20000, gen_loss: 4.876418, disc_loss: 0.151574\n",
      "step: 20500, gen_loss: 5.198801, disc_loss: 0.148342\n",
      "step: 21000, gen_loss: 5.005337, disc_loss: 0.115991\n",
      "step: 21500, gen_loss: 4.250324, disc_loss: 0.277637\n",
      "step: 22000, gen_loss: 2.802020, disc_loss: 0.155145\n",
      "step: 22500, gen_loss: 4.540221, disc_loss: 0.347480\n",
      "step: 23000, gen_loss: 4.836740, disc_loss: 0.181399\n",
      "step: 23500, gen_loss: 4.822203, disc_loss: 0.259780\n",
      "step: 24000, gen_loss: 4.153738, disc_loss: 0.211185\n",
      "step: 24500, gen_loss: 5.048219, disc_loss: 0.138181\n",
      "step: 25000, gen_loss: 5.694863, disc_loss: 0.127033\n",
      "step: 25500, gen_loss: 4.909380, disc_loss: 0.139920\n",
      "step: 26000, gen_loss: 4.118924, disc_loss: 0.121686\n",
      "step: 26500, gen_loss: 3.149289, disc_loss: 0.164285\n",
      "step: 27000, gen_loss: 4.794568, disc_loss: 0.134648\n",
      "step: 27500, gen_loss: 3.868995, disc_loss: 0.084415\n",
      "step: 28000, gen_loss: 5.609795, disc_loss: 0.092755\n",
      "step: 28500, gen_loss: 4.681361, disc_loss: 0.155868\n",
      "step: 29000, gen_loss: 4.668574, disc_loss: 0.181831\n",
      "step: 29500, gen_loss: 3.976046, disc_loss: 0.300730\n",
      "step: 30000, gen_loss: 4.698600, disc_loss: 0.150798\n",
      "step: 30500, gen_loss: 4.901703, disc_loss: 0.173876\n",
      "step: 31000, gen_loss: 4.807575, disc_loss: 0.191634\n",
      "step: 31500, gen_loss: 4.159510, disc_loss: 0.132227\n",
      "step: 32000, gen_loss: 6.076998, disc_loss: 0.080274\n",
      "step: 32500, gen_loss: 6.430903, disc_loss: 0.260322\n",
      "step: 33000, gen_loss: 4.053924, disc_loss: 0.089071\n",
      "step: 33500, gen_loss: 5.639181, disc_loss: 0.089801\n",
      "step: 34000, gen_loss: 5.345060, disc_loss: 0.062155\n",
      "step: 34500, gen_loss: 2.933346, disc_loss: 0.174718\n",
      "step: 35000, gen_loss: 4.096558, disc_loss: 0.079237\n",
      "step: 35500, gen_loss: 4.537305, disc_loss: 0.194836\n",
      "step: 36000, gen_loss: 7.191236, disc_loss: 0.131757\n",
      "step: 36500, gen_loss: 3.308240, disc_loss: 0.180723\n",
      "step: 37000, gen_loss: 4.526470, disc_loss: 0.058153\n",
      "step: 37500, gen_loss: 4.763853, disc_loss: 0.025661\n",
      "step: 38000, gen_loss: 6.600598, disc_loss: 0.097872\n",
      "step: 38500, gen_loss: 6.067770, disc_loss: 0.158674\n",
      "step: 39000, gen_loss: 6.662021, disc_loss: 0.097144\n",
      "step: 39500, gen_loss: 5.541485, disc_loss: 0.077605\n",
      "step: 40000, gen_loss: 3.848447, disc_loss: 0.188015\n",
      "step: 40500, gen_loss: 4.930224, disc_loss: 0.080304\n",
      "step: 41000, gen_loss: 5.676373, disc_loss: 0.080098\n",
      "step: 41500, gen_loss: 5.641461, disc_loss: 0.098764\n",
      "step: 42000, gen_loss: 5.506497, disc_loss: 0.056073\n",
      "step: 42500, gen_loss: 7.462530, disc_loss: 0.159096\n",
      "step: 43000, gen_loss: 4.892241, disc_loss: 0.532518\n",
      "step: 43500, gen_loss: 6.595507, disc_loss: 0.020941\n",
      "step: 44000, gen_loss: 6.211260, disc_loss: 0.078307\n",
      "step: 44500, gen_loss: 5.627349, disc_loss: 0.103281\n",
      "step: 45000, gen_loss: 5.508649, disc_loss: 0.048652\n",
      "step: 45500, gen_loss: 7.068422, disc_loss: 0.077459\n",
      "step: 46000, gen_loss: 7.558779, disc_loss: 0.080327\n",
      "step: 46500, gen_loss: 6.750489, disc_loss: 0.029839\n",
      "step: 47000, gen_loss: 5.430357, disc_loss: 0.081207\n",
      "step: 47500, gen_loss: 5.876296, disc_loss: 0.044034\n",
      "step: 48000, gen_loss: 4.630736, disc_loss: 0.143416\n",
      "step: 48500, gen_loss: 5.720185, disc_loss: 0.092730\n",
      "step: 49000, gen_loss: 6.915287, disc_loss: 0.090224\n",
      "step: 49500, gen_loss: 5.257194, disc_loss: 0.038890\n",
      "step: 50000, gen_loss: 8.722058, disc_loss: 0.052650\n",
      "step: 50500, gen_loss: 8.215571, disc_loss: 0.008620\n",
      "step: 51000, gen_loss: 7.635676, disc_loss: 0.142322\n",
      "step: 51500, gen_loss: 4.501413, disc_loss: 0.116700\n",
      "step: 52000, gen_loss: 4.868709, disc_loss: 0.082619\n",
      "step: 52500, gen_loss: 6.549049, disc_loss: 0.080669\n",
      "step: 53000, gen_loss: 6.346690, disc_loss: 0.082990\n",
      "step: 53500, gen_loss: 7.513562, disc_loss: 0.077929\n",
      "step: 54000, gen_loss: 7.609346, disc_loss: 0.015432\n",
      "step: 54500, gen_loss: 6.879078, disc_loss: 0.097173\n",
      "step: 55000, gen_loss: 6.863007, disc_loss: 0.057919\n",
      "step: 55500, gen_loss: 8.112865, disc_loss: 0.066391\n",
      "step: 56000, gen_loss: 7.420357, disc_loss: 0.017807\n",
      "step: 56500, gen_loss: 2.491294, disc_loss: 0.016703\n",
      "step: 57000, gen_loss: 6.563250, disc_loss: 0.020256\n",
      "step: 57500, gen_loss: 7.388284, disc_loss: 0.018289\n",
      "step: 58000, gen_loss: 8.234947, disc_loss: 0.038141\n",
      "step: 58500, gen_loss: 6.324577, disc_loss: 0.066223\n",
      "step: 59000, gen_loss: 6.998763, disc_loss: 0.140082\n",
      "step: 59500, gen_loss: 6.549025, disc_loss: 0.058871\n",
      "step: 60000, gen_loss: 6.840778, disc_loss: 0.277729\n",
      "step: 60500, gen_loss: 7.266601, disc_loss: 0.039837\n",
      "step: 61000, gen_loss: 6.458753, disc_loss: 0.208130\n",
      "step: 61500, gen_loss: 8.181312, disc_loss: 0.047563\n",
      "step: 62000, gen_loss: 7.632077, disc_loss: 0.190328\n",
      "step: 62500, gen_loss: 8.301777, disc_loss: 0.061356\n",
      "step: 63000, gen_loss: 8.249866, disc_loss: 0.044313\n",
      "step: 63500, gen_loss: 5.307097, disc_loss: 0.114605\n",
      "step: 64000, gen_loss: 7.553109, disc_loss: 0.080454\n",
      "step: 64500, gen_loss: 5.573301, disc_loss: 0.108160\n",
      "step: 65000, gen_loss: 7.733584, disc_loss: 0.114959\n",
      "step: 65500, gen_loss: 10.225343, disc_loss: 0.109881\n",
      "step: 66000, gen_loss: 7.411716, disc_loss: 0.025458\n",
      "step: 66500, gen_loss: 7.718226, disc_loss: 0.070488\n",
      "step: 67000, gen_loss: 6.751156, disc_loss: 0.250533\n",
      "step: 67500, gen_loss: 6.996140, disc_loss: 0.113949\n",
      "step: 68000, gen_loss: 7.444713, disc_loss: 0.046543\n",
      "step: 68500, gen_loss: 6.190851, disc_loss: 0.054372\n",
      "step: 69000, gen_loss: 3.575006, disc_loss: 0.157575\n",
      "step: 69500, gen_loss: 7.915603, disc_loss: 0.090106\n",
      "step: 70000, gen_loss: 8.329928, disc_loss: 0.164285\n",
      "step: 70500, gen_loss: 6.716043, disc_loss: 0.062082\n",
      "step: 71000, gen_loss: 8.208860, disc_loss: 0.048677\n",
      "step: 71500, gen_loss: 8.777609, disc_loss: 0.139692\n",
      "step: 72000, gen_loss: 6.390849, disc_loss: 0.036562\n",
      "step: 72500, gen_loss: 7.954193, disc_loss: 0.054049\n",
      "step: 73000, gen_loss: 6.938149, disc_loss: 0.040788\n",
      "step: 73500, gen_loss: 6.951823, disc_loss: 0.017740\n",
      "step: 74000, gen_loss: 3.794082, disc_loss: 0.267328\n",
      "step: 74500, gen_loss: 6.684215, disc_loss: 0.009515\n",
      "step: 75000, gen_loss: 6.170237, disc_loss: 0.138214\n",
      "step: 75500, gen_loss: 8.686186, disc_loss: 0.052243\n",
      "step: 76000, gen_loss: 9.371739, disc_loss: 0.169452\n",
      "step: 76500, gen_loss: 7.039939, disc_loss: 0.088790\n",
      "step: 77000, gen_loss: 7.194175, disc_loss: 0.049157\n",
      "step: 77500, gen_loss: 5.093815, disc_loss: 0.149162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 78000, gen_loss: 8.843852, disc_loss: 0.019679\n",
      "step: 78500, gen_loss: 6.739763, disc_loss: 0.083419\n",
      "step: 79000, gen_loss: 7.799727, disc_loss: 0.068565\n",
      "step: 79500, gen_loss: 4.488023, disc_loss: 0.111474\n",
      "step: 80000, gen_loss: 6.476758, disc_loss: 0.059482\n",
      "step: 80500, gen_loss: 7.171435, disc_loss: 0.028724\n",
      "step: 81000, gen_loss: 7.500202, disc_loss: 0.041686\n",
      "step: 81500, gen_loss: 6.546262, disc_loss: 0.079863\n",
      "step: 82000, gen_loss: 8.004476, disc_loss: 0.095318\n",
      "step: 82500, gen_loss: 6.678963, disc_loss: 0.056194\n",
      "step: 83000, gen_loss: 6.478731, disc_loss: 0.134609\n",
      "step: 83500, gen_loss: 9.401062, disc_loss: 0.625799\n",
      "step: 84000, gen_loss: 7.510420, disc_loss: 0.011738\n",
      "step: 84500, gen_loss: 7.227165, disc_loss: 0.020759\n",
      "step: 85000, gen_loss: 8.217575, disc_loss: 0.119706\n",
      "step: 85500, gen_loss: 8.694629, disc_loss: 0.014443\n",
      "step: 86000, gen_loss: 7.416728, disc_loss: 0.090740\n",
      "step: 86500, gen_loss: 8.657449, disc_loss: 0.019847\n",
      "step: 87000, gen_loss: 6.964628, disc_loss: 0.040910\n",
      "step: 87500, gen_loss: 7.851440, disc_loss: 0.017549\n",
      "step: 88000, gen_loss: 5.595195, disc_loss: 0.063954\n",
      "step: 88500, gen_loss: 9.595007, disc_loss: 0.011775\n",
      "step: 89000, gen_loss: 7.853581, disc_loss: 0.041392\n",
      "step: 89500, gen_loss: 6.940600, disc_loss: 0.093632\n",
      "step: 90000, gen_loss: 7.887924, disc_loss: 0.014926\n",
      "step: 90500, gen_loss: 8.034519, disc_loss: 0.038424\n",
      "step: 91000, gen_loss: 6.514193, disc_loss: 0.093605\n",
      "step: 91500, gen_loss: 8.480123, disc_loss: 0.035320\n",
      "step: 92000, gen_loss: 7.773535, disc_loss: 0.028601\n",
      "step: 92500, gen_loss: 9.039784, disc_loss: 0.019844\n",
      "step: 93000, gen_loss: 7.706318, disc_loss: 0.022278\n",
      "step: 93500, gen_loss: 4.894417, disc_loss: 0.099999\n",
      "step: 94000, gen_loss: 6.221189, disc_loss: 0.003288\n",
      "step: 94500, gen_loss: 5.360830, disc_loss: 0.047693\n",
      "step: 95000, gen_loss: 4.275179, disc_loss: 0.052619\n",
      "step: 95500, gen_loss: 6.161265, disc_loss: 0.016037\n",
      "step: 96000, gen_loss: 5.540874, disc_loss: 0.018533\n",
      "step: 96500, gen_loss: 7.959295, disc_loss: 0.061324\n",
      "step: 97000, gen_loss: 9.195937, disc_loss: 0.028717\n",
      "step: 97500, gen_loss: 7.528731, disc_loss: 0.248215\n",
      "step: 98000, gen_loss: 7.081107, disc_loss: 0.041659\n",
      "step: 98500, gen_loss: 10.604266, disc_loss: 0.300983\n",
      "step: 99000, gen_loss: 7.364538, disc_loss: 0.015347\n",
      "step: 99500, gen_loss: 7.280520, disc_loss: 0.052759\n",
      "step: 100000, gen_loss: 7.484982, disc_loss: 0.024151\n",
      "step: 100500, gen_loss: 10.611494, disc_loss: 0.056476\n",
      "step: 101000, gen_loss: 8.516675, disc_loss: 0.009947\n",
      "step: 101500, gen_loss: 8.157562, disc_loss: 0.115681\n",
      "step: 102000, gen_loss: 7.402368, disc_loss: 0.102917\n",
      "step: 102500, gen_loss: 8.713721, disc_loss: 0.017013\n",
      "step: 103000, gen_loss: 8.478838, disc_loss: 0.026137\n",
      "step: 103500, gen_loss: 8.054696, disc_loss: 0.035261\n",
      "step: 104000, gen_loss: 7.517237, disc_loss: 0.052131\n",
      "step: 104500, gen_loss: 5.792449, disc_loss: 0.200142\n",
      "step: 105000, gen_loss: 9.415640, disc_loss: 0.008014\n",
      "step: 105500, gen_loss: 6.567407, disc_loss: 0.027065\n",
      "step: 106000, gen_loss: 7.708495, disc_loss: 0.005985\n",
      "step: 106500, gen_loss: 9.829493, disc_loss: 0.005454\n",
      "step: 107000, gen_loss: 7.343593, disc_loss: 0.027290\n",
      "step: 107500, gen_loss: 5.402482, disc_loss: 0.066480\n",
      "step: 108000, gen_loss: 8.920897, disc_loss: 0.003175\n",
      "step: 108500, gen_loss: 6.829885, disc_loss: 0.052226\n",
      "step: 109000, gen_loss: 6.770004, disc_loss: 0.178004\n",
      "step: 109500, gen_loss: 6.429202, disc_loss: 0.026829\n",
      "step: 110000, gen_loss: 9.195214, disc_loss: 0.025254\n",
      "step: 110500, gen_loss: 7.079411, disc_loss: 0.017305\n",
      "step: 111000, gen_loss: 7.691765, disc_loss: 0.008620\n",
      "step: 111500, gen_loss: 7.811482, disc_loss: 0.021835\n",
      "step: 112000, gen_loss: 8.182611, disc_loss: 0.064787\n",
      "step: 112500, gen_loss: 7.268859, disc_loss: 0.157231\n",
      "step: 113000, gen_loss: 8.203952, disc_loss: 0.024046\n",
      "step: 113500, gen_loss: 7.948986, disc_loss: 0.071388\n",
      "step: 114000, gen_loss: 8.609560, disc_loss: 0.048094\n",
      "step: 114500, gen_loss: 8.052849, disc_loss: 0.016624\n",
      "step: 115000, gen_loss: 9.837191, disc_loss: 0.018388\n",
      "step: 115500, gen_loss: 8.793970, disc_loss: 0.051566\n",
      "step: 116000, gen_loss: 7.270066, disc_loss: 0.012864\n",
      "step: 116500, gen_loss: 9.913345, disc_loss: 0.018701\n",
      "step: 117000, gen_loss: 8.617314, disc_loss: 0.037957\n",
      "step: 117500, gen_loss: 8.178912, disc_loss: 0.016585\n",
      "step: 118000, gen_loss: 9.381388, disc_loss: 0.002226\n",
      "step: 118500, gen_loss: 8.597996, disc_loss: 0.011288\n",
      "step: 119000, gen_loss: 7.280868, disc_loss: 0.058281\n",
      "step: 119500, gen_loss: 8.861977, disc_loss: 0.027087\n",
      "step: 120000, gen_loss: 8.164781, disc_loss: 0.015434\n",
      "step: 120500, gen_loss: 7.871373, disc_loss: 0.115698\n",
      "step: 121000, gen_loss: 9.666409, disc_loss: 0.060441\n",
      "step: 121500, gen_loss: 8.348432, disc_loss: 0.011577\n",
      "step: 122000, gen_loss: 7.303197, disc_loss: 0.164193\n",
      "step: 122500, gen_loss: 9.989613, disc_loss: 0.038880\n",
      "step: 123000, gen_loss: 10.578608, disc_loss: 0.076037\n",
      "step: 123500, gen_loss: 8.175715, disc_loss: 0.010155\n",
      "step: 124000, gen_loss: 7.977258, disc_loss: 0.032769\n",
      "step: 124500, gen_loss: 9.577598, disc_loss: 0.049555\n",
      "step: 125000, gen_loss: 8.245999, disc_loss: 0.030771\n",
      "step: 125500, gen_loss: 8.106399, disc_loss: 0.035643\n",
      "step: 126000, gen_loss: 6.762468, disc_loss: 0.006626\n",
      "step: 126500, gen_loss: 9.208897, disc_loss: 0.103522\n",
      "step: 127000, gen_loss: 8.305266, disc_loss: 0.099105\n",
      "step: 127500, gen_loss: 10.372544, disc_loss: 0.024485\n",
      "step: 128000, gen_loss: 9.603881, disc_loss: 0.223416\n",
      "step: 128500, gen_loss: 10.409940, disc_loss: 0.021111\n",
      "step: 129000, gen_loss: 9.505355, disc_loss: 0.013340\n",
      "step: 129500, gen_loss: 9.164577, disc_loss: 0.058894\n",
      "step: 130000, gen_loss: 9.646807, disc_loss: 0.015336\n",
      "step: 130500, gen_loss: 8.226447, disc_loss: 0.016363\n",
      "step: 131000, gen_loss: 4.998781, disc_loss: 0.042090\n",
      "step: 131500, gen_loss: 10.702286, disc_loss: 0.084077\n",
      "step: 132000, gen_loss: 8.756765, disc_loss: 0.017992\n",
      "step: 132500, gen_loss: 7.587901, disc_loss: 0.036879\n",
      "step: 133000, gen_loss: 9.884966, disc_loss: 0.054583\n",
      "step: 133500, gen_loss: 9.628183, disc_loss: 0.013128\n",
      "step: 134000, gen_loss: 7.369473, disc_loss: 0.003345\n",
      "step: 134500, gen_loss: 6.781569, disc_loss: 0.055329\n",
      "step: 135000, gen_loss: 7.546712, disc_loss: 0.041062\n",
      "step: 135500, gen_loss: 8.368156, disc_loss: 0.037060\n",
      "step: 136000, gen_loss: 11.048631, disc_loss: 0.011104\n",
      "step: 136500, gen_loss: 8.558319, disc_loss: 0.054891\n",
      "step: 137000, gen_loss: 7.634640, disc_loss: 0.028704\n",
      "step: 137500, gen_loss: 7.505512, disc_loss: 0.004346\n",
      "step: 138000, gen_loss: 8.610627, disc_loss: 0.145534\n",
      "step: 138500, gen_loss: 7.674533, disc_loss: 0.026052\n",
      "step: 139000, gen_loss: 6.914991, disc_loss: 0.130361\n",
      "step: 139500, gen_loss: 8.231939, disc_loss: 0.056015\n",
      "step: 140000, gen_loss: 9.469381, disc_loss: 0.009404\n",
      "step: 140500, gen_loss: 8.668232, disc_loss: 0.112497\n",
      "step: 141000, gen_loss: 7.397928, disc_loss: 0.062285\n",
      "step: 141500, gen_loss: 8.437392, disc_loss: 0.047121\n",
      "step: 142000, gen_loss: 6.967225, disc_loss: 0.165505\n",
      "step: 142500, gen_loss: 8.142809, disc_loss: 0.009680\n",
      "step: 143000, gen_loss: 10.798663, disc_loss: 0.045904\n",
      "step: 143500, gen_loss: 7.895945, disc_loss: 0.181584\n",
      "step: 144000, gen_loss: 7.030949, disc_loss: 0.010648\n",
      "step: 144500, gen_loss: 10.365509, disc_loss: 0.001153\n",
      "step: 145000, gen_loss: 8.629740, disc_loss: 0.166981\n",
      "step: 145500, gen_loss: 6.223282, disc_loss: 0.062676\n",
      "step: 146000, gen_loss: 10.015944, disc_loss: 0.011421\n",
      "step: 146500, gen_loss: 9.014462, disc_loss: 0.010175\n",
      "step: 147000, gen_loss: 9.303662, disc_loss: 0.005589\n",
      "step: 147500, gen_loss: 9.906835, disc_loss: 0.042724\n",
      "step: 148000, gen_loss: 7.636297, disc_loss: 0.001684\n",
      "step: 148500, gen_loss: 8.037898, disc_loss: 0.049338\n",
      "step: 149000, gen_loss: 8.259231, disc_loss: 0.002093\n",
      "step: 149500, gen_loss: 9.129440, disc_loss: 0.065027\n",
      "step: 150000, gen_loss: 7.780201, disc_loss: 0.019484\n",
      "step: 150500, gen_loss: 8.695847, disc_loss: 0.037731\n",
      "step: 151000, gen_loss: 8.265955, disc_loss: 0.055587\n",
      "step: 151500, gen_loss: 11.708417, disc_loss: 0.096522\n",
      "step: 152000, gen_loss: 9.949538, disc_loss: 0.139946\n",
      "step: 152500, gen_loss: 11.119885, disc_loss: 0.005888\n",
      "step: 153000, gen_loss: 12.084161, disc_loss: 0.055419\n",
      "step: 153500, gen_loss: 9.934430, disc_loss: 0.094196\n",
      "step: 154000, gen_loss: 8.519577, disc_loss: 0.052385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 154500, gen_loss: 7.672040, disc_loss: 0.019351\n",
      "step: 155000, gen_loss: 11.590611, disc_loss: 0.038642\n",
      "step: 155500, gen_loss: 10.434713, disc_loss: 0.208951\n",
      "step: 156000, gen_loss: 5.882378, disc_loss: 0.057209\n",
      "step: 156500, gen_loss: 8.133858, disc_loss: 0.005268\n",
      "step: 157000, gen_loss: 9.227178, disc_loss: 0.057564\n",
      "step: 157500, gen_loss: 8.770419, disc_loss: 0.074017\n",
      "step: 158000, gen_loss: 7.960136, disc_loss: 0.002913\n",
      "step: 158500, gen_loss: 5.480932, disc_loss: 0.045572\n",
      "step: 159000, gen_loss: 6.683920, disc_loss: 0.009258\n",
      "step: 159500, gen_loss: 8.033403, disc_loss: 0.041182\n",
      "step: 160000, gen_loss: 8.982258, disc_loss: 0.028858\n",
      "step: 160500, gen_loss: 10.013349, disc_loss: 0.007153\n",
      "step: 161000, gen_loss: 10.176013, disc_loss: 0.210886\n",
      "step: 161500, gen_loss: 8.480699, disc_loss: 0.001047\n",
      "step: 162000, gen_loss: 7.198854, disc_loss: 0.004459\n",
      "step: 162500, gen_loss: 8.965384, disc_loss: 0.042665\n",
      "step: 163000, gen_loss: 10.630520, disc_loss: 0.046294\n",
      "step: 163500, gen_loss: 10.600250, disc_loss: 0.002511\n",
      "step: 164000, gen_loss: 8.711501, disc_loss: 0.059823\n",
      "step: 164500, gen_loss: 8.882392, disc_loss: 0.009708\n",
      "step: 165000, gen_loss: 7.913970, disc_loss: 0.018336\n",
      "step: 165500, gen_loss: 6.911539, disc_loss: 0.092391\n",
      "step: 166000, gen_loss: 9.435596, disc_loss: 0.016302\n",
      "step: 166500, gen_loss: 9.623672, disc_loss: 0.013273\n",
      "step: 167000, gen_loss: 9.253364, disc_loss: 0.016145\n",
      "step: 167500, gen_loss: 9.172105, disc_loss: 0.267912\n",
      "step: 168000, gen_loss: 10.237950, disc_loss: 0.005517\n",
      "step: 168500, gen_loss: 9.003165, disc_loss: 0.075754\n",
      "step: 169000, gen_loss: 11.521216, disc_loss: 0.002778\n",
      "step: 169500, gen_loss: 10.793627, disc_loss: 0.098212\n",
      "step: 170000, gen_loss: 9.377388, disc_loss: 0.016457\n",
      "step: 170500, gen_loss: 10.385519, disc_loss: 0.002876\n",
      "step: 171000, gen_loss: 8.383102, disc_loss: 0.002840\n",
      "step: 171500, gen_loss: 10.555916, disc_loss: 0.007856\n",
      "step: 172000, gen_loss: 7.853070, disc_loss: 0.014900\n",
      "step: 172500, gen_loss: 8.524298, disc_loss: 0.025388\n",
      "step: 173000, gen_loss: 8.583910, disc_loss: 0.003472\n",
      "step: 173500, gen_loss: 7.816677, disc_loss: 0.121317\n",
      "step: 174000, gen_loss: 8.735727, disc_loss: 0.021376\n",
      "step: 174500, gen_loss: 11.075331, disc_loss: 0.005945\n",
      "step: 175000, gen_loss: 6.098312, disc_loss: 0.010078\n",
      "step: 175500, gen_loss: 7.789285, disc_loss: 0.006100\n",
      "step: 176000, gen_loss: 8.954512, disc_loss: 0.016630\n",
      "step: 176500, gen_loss: 8.570724, disc_loss: 0.088742\n",
      "step: 177000, gen_loss: 10.396227, disc_loss: 0.003261\n",
      "step: 177500, gen_loss: 6.300394, disc_loss: 0.032246\n",
      "step: 178000, gen_loss: 9.780974, disc_loss: 0.027623\n",
      "step: 178500, gen_loss: 10.398550, disc_loss: 0.031733\n",
      "step: 179000, gen_loss: 10.117152, disc_loss: 0.057636\n",
      "step: 179500, gen_loss: 10.156240, disc_loss: 0.006777\n",
      "step: 180000, gen_loss: 10.769972, disc_loss: 0.011350\n",
      "step: 180500, gen_loss: 10.068091, disc_loss: 0.062066\n",
      "step: 181000, gen_loss: 10.659206, disc_loss: 0.002524\n",
      "step: 181500, gen_loss: 10.972507, disc_loss: 0.035387\n",
      "step: 182000, gen_loss: 10.466646, disc_loss: 0.009365\n",
      "step: 182500, gen_loss: 11.498219, disc_loss: 0.016498\n",
      "step: 183000, gen_loss: 8.050475, disc_loss: 0.101534\n",
      "step: 183500, gen_loss: 10.065749, disc_loss: 0.032247\n",
      "step: 184000, gen_loss: 8.295783, disc_loss: 0.011315\n",
      "step: 184500, gen_loss: 11.500193, disc_loss: 0.040232\n",
      "step: 185000, gen_loss: 10.026525, disc_loss: 0.018273\n",
      "step: 185500, gen_loss: 10.618374, disc_loss: 0.220846\n",
      "step: 186000, gen_loss: 8.563616, disc_loss: 0.014974\n",
      "step: 186500, gen_loss: 9.101625, disc_loss: 0.051058\n",
      "step: 187000, gen_loss: 5.824332, disc_loss: 0.065380\n",
      "step: 187500, gen_loss: 8.853685, disc_loss: 0.034186\n",
      "step: 188000, gen_loss: 8.586460, disc_loss: 0.023942\n",
      "step: 188500, gen_loss: 10.875853, disc_loss: 0.006944\n",
      "step: 189000, gen_loss: 8.198196, disc_loss: 0.026856\n",
      "step: 189500, gen_loss: 9.623344, disc_loss: 0.007556\n",
      "step: 190000, gen_loss: 9.526496, disc_loss: 0.024414\n",
      "step: 190500, gen_loss: 8.503687, disc_loss: 0.069289\n",
      "step: 191000, gen_loss: 6.843872, disc_loss: 0.103105\n",
      "step: 191500, gen_loss: 3.966485, disc_loss: 0.088989\n",
      "step: 192000, gen_loss: 7.363637, disc_loss: 0.005737\n",
      "step: 192500, gen_loss: 9.213995, disc_loss: 0.004866\n",
      "step: 193000, gen_loss: 10.404344, disc_loss: 0.003770\n",
      "step: 193500, gen_loss: 10.524910, disc_loss: 0.011921\n",
      "step: 194000, gen_loss: 12.678924, disc_loss: 0.006107\n",
      "step: 194500, gen_loss: 10.848452, disc_loss: 0.007035\n",
      "step: 195000, gen_loss: 7.220267, disc_loss: 0.029467\n",
      "step: 195500, gen_loss: 9.379746, disc_loss: 0.054070\n",
      "step: 196000, gen_loss: 9.953979, disc_loss: 0.016303\n",
      "step: 196500, gen_loss: 9.351904, disc_loss: 0.113158\n",
      "step: 197000, gen_loss: 8.371051, disc_loss: 0.014460\n",
      "step: 197500, gen_loss: 7.082027, disc_loss: 0.021010\n",
      "step: 198000, gen_loss: 7.322411, disc_loss: 0.150054\n",
      "step: 198500, gen_loss: 6.532872, disc_loss: 0.034765\n",
      "step: 199000, gen_loss: 8.843712, disc_loss: 0.004951\n",
      "step: 199500, gen_loss: 10.213182, disc_loss: 0.011157\n",
      "step: 200000, gen_loss: 7.214729, disc_loss: 0.021115\n",
      "step: 200500, gen_loss: 8.594402, disc_loss: 0.042010\n",
      "step: 201000, gen_loss: 9.869424, disc_loss: 0.098353\n",
      "step: 201500, gen_loss: 10.298359, disc_loss: 0.004569\n",
      "step: 202000, gen_loss: 8.799768, disc_loss: 0.192147\n",
      "step: 202500, gen_loss: 7.281222, disc_loss: 0.019880\n",
      "step: 203000, gen_loss: 11.444853, disc_loss: 0.005101\n",
      "step: 203500, gen_loss: 9.154491, disc_loss: 0.001942\n",
      "step: 204000, gen_loss: 8.170767, disc_loss: 0.122582\n",
      "step: 204500, gen_loss: 11.562252, disc_loss: 0.006453\n",
      "step: 205000, gen_loss: 13.207916, disc_loss: 0.051807\n",
      "step: 205500, gen_loss: 8.030432, disc_loss: 0.008344\n",
      "step: 206000, gen_loss: 12.059466, disc_loss: 0.092127\n",
      "step: 206500, gen_loss: 10.335232, disc_loss: 0.060534\n",
      "step: 207000, gen_loss: 9.814199, disc_loss: 0.021998\n",
      "step: 207500, gen_loss: 11.617305, disc_loss: 0.008016\n",
      "step: 208000, gen_loss: 6.812070, disc_loss: 0.002278\n",
      "step: 208500, gen_loss: 8.638447, disc_loss: 0.004795\n",
      "step: 209000, gen_loss: 16.640064, disc_loss: 0.037461\n",
      "step: 209500, gen_loss: 16.529934, disc_loss: 0.001117\n",
      "step: 210000, gen_loss: 21.069901, disc_loss: 0.000146\n",
      "step: 210500, gen_loss: 25.462082, disc_loss: 0.010605\n",
      "step: 211000, gen_loss: 19.754467, disc_loss: 0.155370\n",
      "step: 211500, gen_loss: 21.079292, disc_loss: 0.000133\n",
      "step: 212000, gen_loss: 20.845806, disc_loss: 0.000169\n",
      "step: 212500, gen_loss: 21.230766, disc_loss: 0.000025\n",
      "step: 213000, gen_loss: 19.726313, disc_loss: 0.000327\n",
      "step: 213500, gen_loss: 16.409492, disc_loss: 0.000008\n",
      "step: 214000, gen_loss: 16.597090, disc_loss: 0.000044\n",
      "step: 214500, gen_loss: 16.581408, disc_loss: 0.000846\n",
      "step: 215000, gen_loss: 15.987698, disc_loss: 0.000100\n",
      "step: 215500, gen_loss: 21.004974, disc_loss: 0.000028\n",
      "step: 216000, gen_loss: 19.054201, disc_loss: 0.000452\n",
      "step: 216500, gen_loss: 17.448196, disc_loss: 0.000021\n",
      "step: 217000, gen_loss: 24.079506, disc_loss: 0.000062\n",
      "step: 217500, gen_loss: 17.140543, disc_loss: 0.000188\n",
      "step: 218000, gen_loss: 18.669621, disc_loss: 0.000025\n",
      "step: 218500, gen_loss: 21.808321, disc_loss: 0.000138\n",
      "step: 219000, gen_loss: 19.775919, disc_loss: 0.000017\n",
      "step: 219500, gen_loss: 19.888466, disc_loss: 0.000035\n",
      "step: 220000, gen_loss: 21.990526, disc_loss: 0.000000\n",
      "step: 220500, gen_loss: 20.889458, disc_loss: 0.000020\n",
      "step: 221000, gen_loss: 24.592081, disc_loss: 0.000006\n",
      "step: 221500, gen_loss: 21.579113, disc_loss: 0.000002\n",
      "step: 222000, gen_loss: 24.331556, disc_loss: 0.000055\n",
      "step: 222500, gen_loss: 25.455448, disc_loss: 0.000517\n",
      "step: 223000, gen_loss: 23.090868, disc_loss: 0.000011\n",
      "step: 223500, gen_loss: 24.735720, disc_loss: 0.000003\n",
      "step: 224000, gen_loss: 21.698002, disc_loss: 0.000036\n",
      "step: 224500, gen_loss: 25.038366, disc_loss: 0.000036\n",
      "step: 225000, gen_loss: 22.735668, disc_loss: 0.000028\n",
      "step: 225500, gen_loss: 20.763660, disc_loss: 0.000008\n",
      "step: 226000, gen_loss: 20.924528, disc_loss: 0.000001\n",
      "step: 226500, gen_loss: 18.222187, disc_loss: 0.000058\n",
      "step: 227000, gen_loss: 18.222008, disc_loss: 0.000005\n",
      "step: 227500, gen_loss: 29.700642, disc_loss: 0.000010\n",
      "step: 228000, gen_loss: 28.901009, disc_loss: 0.000002\n",
      "step: 228500, gen_loss: 29.569113, disc_loss: 0.000011\n",
      "step: 229000, gen_loss: 29.084785, disc_loss: 0.000000\n",
      "step: 229500, gen_loss: 24.934536, disc_loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 230000, gen_loss: 23.786026, disc_loss: 0.000000\n",
      "step: 230500, gen_loss: 23.036171, disc_loss: 0.000000\n",
      "step: 231000, gen_loss: 26.636299, disc_loss: 0.000152\n",
      "step: 231500, gen_loss: 27.935608, disc_loss: 0.000000\n",
      "step: 232000, gen_loss: 26.479084, disc_loss: 0.000000\n",
      "step: 232500, gen_loss: 25.840218, disc_loss: 0.000000\n",
      "step: 233000, gen_loss: 27.626308, disc_loss: 0.009219\n",
      "step: 233500, gen_loss: 22.943275, disc_loss: 0.000051\n",
      "step: 234000, gen_loss: 25.230194, disc_loss: 0.000000\n",
      "step: 234500, gen_loss: 24.783421, disc_loss: 0.000000\n",
      "step: 235000, gen_loss: 21.804007, disc_loss: 0.000001\n",
      "step: 235500, gen_loss: 26.029070, disc_loss: 0.000002\n",
      "step: 236000, gen_loss: 23.966883, disc_loss: 0.000019\n",
      "step: 236500, gen_loss: 22.085133, disc_loss: 0.000004\n",
      "step: 237000, gen_loss: 22.047390, disc_loss: 0.000000\n",
      "step: 237500, gen_loss: 27.442329, disc_loss: 0.000001\n",
      "step: 238000, gen_loss: 23.290649, disc_loss: 0.001122\n",
      "step: 238500, gen_loss: 26.995605, disc_loss: 0.000001\n",
      "step: 239000, gen_loss: 28.429659, disc_loss: 0.000000\n",
      "step: 239500, gen_loss: 25.434652, disc_loss: 0.000013\n",
      "step: 240000, gen_loss: 26.040833, disc_loss: 0.000000\n",
      "step: 240500, gen_loss: 24.724546, disc_loss: 0.005106\n",
      "step: 241000, gen_loss: 27.030920, disc_loss: 0.000063\n",
      "step: 241500, gen_loss: 26.232456, disc_loss: 0.000483\n",
      "step: 242000, gen_loss: 25.218966, disc_loss: 0.000003\n",
      "step: 242500, gen_loss: 22.940939, disc_loss: 0.000010\n",
      "step: 243000, gen_loss: 27.170822, disc_loss: 0.001937\n",
      "step: 243500, gen_loss: 29.611580, disc_loss: 0.006401\n",
      "step: 244000, gen_loss: 13.748984, disc_loss: 0.067612\n",
      "step: 244500, gen_loss: 10.926167, disc_loss: 0.003064\n",
      "step: 245000, gen_loss: 6.539176, disc_loss: 0.174458\n",
      "step: 245500, gen_loss: 7.652877, disc_loss: 0.028002\n",
      "step: 246000, gen_loss: 5.946981, disc_loss: 0.039995\n",
      "step: 246500, gen_loss: 5.831255, disc_loss: 0.029198\n",
      "step: 247000, gen_loss: 6.005670, disc_loss: 0.319295\n",
      "step: 247500, gen_loss: 5.432625, disc_loss: 0.030619\n",
      "step: 248000, gen_loss: 6.340418, disc_loss: 0.048897\n",
      "step: 248500, gen_loss: 9.057147, disc_loss: 0.048523\n",
      "step: 249000, gen_loss: 6.520897, disc_loss: 0.041451\n",
      "step: 249500, gen_loss: 5.289801, disc_loss: 0.086423\n",
      "step: 250000, gen_loss: 6.552005, disc_loss: 0.064083\n",
      "step: 250500, gen_loss: 7.723040, disc_loss: 0.042608\n",
      "step: 251000, gen_loss: 6.633176, disc_loss: 0.041630\n",
      "step: 251500, gen_loss: 8.090706, disc_loss: 0.011352\n",
      "step: 252000, gen_loss: 10.369202, disc_loss: 0.095994\n",
      "step: 252500, gen_loss: 6.411976, disc_loss: 0.016991\n",
      "step: 253000, gen_loss: 9.454205, disc_loss: 0.027676\n",
      "step: 253500, gen_loss: 8.893951, disc_loss: 0.025163\n",
      "step: 254000, gen_loss: 8.537697, disc_loss: 0.095041\n",
      "step: 254500, gen_loss: 7.705029, disc_loss: 0.012022\n",
      "step: 255000, gen_loss: 9.246432, disc_loss: 0.042710\n",
      "step: 255500, gen_loss: 6.225214, disc_loss: 0.015687\n",
      "step: 256000, gen_loss: 8.286551, disc_loss: 0.034805\n",
      "step: 256500, gen_loss: 8.149151, disc_loss: 0.046287\n",
      "step: 257000, gen_loss: 5.519951, disc_loss: 0.004850\n",
      "step: 257500, gen_loss: 7.336620, disc_loss: 0.024648\n",
      "step: 258000, gen_loss: 8.631128, disc_loss: 0.026245\n",
      "step: 258500, gen_loss: 7.390324, disc_loss: 0.012409\n",
      "step: 259000, gen_loss: 8.593893, disc_loss: 0.019137\n",
      "step: 259500, gen_loss: 7.492796, disc_loss: 0.006600\n",
      "step: 260000, gen_loss: 7.219882, disc_loss: 0.002819\n",
      "step: 260500, gen_loss: 8.657213, disc_loss: 0.010803\n",
      "step: 261000, gen_loss: 8.267945, disc_loss: 0.011791\n",
      "step: 261500, gen_loss: 9.884801, disc_loss: 0.009810\n",
      "step: 262000, gen_loss: 7.685921, disc_loss: 0.028933\n",
      "step: 262500, gen_loss: 10.533438, disc_loss: 0.008747\n",
      "step: 263000, gen_loss: 10.004839, disc_loss: 0.030286\n",
      "step: 263500, gen_loss: 10.197166, disc_loss: 0.001245\n",
      "step: 264000, gen_loss: 10.342928, disc_loss: 0.045935\n",
      "step: 264500, gen_loss: 11.369612, disc_loss: 0.035469\n",
      "step: 265000, gen_loss: 8.483862, disc_loss: 0.009026\n",
      "step: 265500, gen_loss: 6.175097, disc_loss: 0.159782\n",
      "step: 266000, gen_loss: 8.702770, disc_loss: 0.106160\n",
      "step: 266500, gen_loss: 10.837742, disc_loss: 0.013551\n",
      "step: 267000, gen_loss: 8.184369, disc_loss: 0.019885\n",
      "step: 267500, gen_loss: 9.893697, disc_loss: 0.011540\n",
      "step: 268000, gen_loss: 8.687857, disc_loss: 0.082576\n",
      "step: 268500, gen_loss: 10.058549, disc_loss: 0.007085\n",
      "step: 269000, gen_loss: 10.775027, disc_loss: 0.010984\n",
      "step: 269500, gen_loss: 9.038164, disc_loss: 0.012585\n",
      "step: 270000, gen_loss: 10.340654, disc_loss: 0.026939\n",
      "step: 270500, gen_loss: 9.839554, disc_loss: 0.220299\n",
      "step: 271000, gen_loss: 10.007658, disc_loss: 0.010484\n",
      "step: 271500, gen_loss: 10.988472, disc_loss: 0.026826\n",
      "step: 272000, gen_loss: 11.356091, disc_loss: 0.017364\n",
      "step: 272500, gen_loss: 8.172644, disc_loss: 0.072119\n",
      "step: 273000, gen_loss: 10.644204, disc_loss: 0.002134\n",
      "step: 273500, gen_loss: 10.759336, disc_loss: 0.059287\n",
      "step: 274000, gen_loss: 8.478710, disc_loss: 0.003287\n",
      "step: 274500, gen_loss: 10.729525, disc_loss: 0.020020\n",
      "step: 275000, gen_loss: 10.939859, disc_loss: 0.029459\n",
      "step: 275500, gen_loss: 8.836081, disc_loss: 0.004864\n",
      "step: 276000, gen_loss: 8.942937, disc_loss: 0.028040\n",
      "step: 276500, gen_loss: 8.164566, disc_loss: 0.037451\n",
      "step: 277000, gen_loss: 10.448227, disc_loss: 0.006945\n",
      "step: 277500, gen_loss: 7.362742, disc_loss: 0.013618\n",
      "step: 278000, gen_loss: 5.626600, disc_loss: 0.206358\n",
      "step: 278500, gen_loss: 12.089865, disc_loss: 0.024339\n",
      "step: 279000, gen_loss: 9.514995, disc_loss: 0.006977\n",
      "step: 279500, gen_loss: 8.709130, disc_loss: 0.002492\n",
      "step: 280000, gen_loss: 8.224119, disc_loss: 0.022073\n",
      "step: 280500, gen_loss: 11.249460, disc_loss: 0.023813\n",
      "step: 281000, gen_loss: 7.001450, disc_loss: 0.005921\n",
      "step: 281500, gen_loss: 11.274967, disc_loss: 0.003271\n",
      "step: 282000, gen_loss: 7.554229, disc_loss: 0.006632\n",
      "step: 282500, gen_loss: 10.131556, disc_loss: 0.005137\n",
      "step: 283000, gen_loss: 10.766567, disc_loss: 0.015442\n",
      "step: 283500, gen_loss: 10.556789, disc_loss: 0.013868\n",
      "step: 284000, gen_loss: 10.504374, disc_loss: 0.002565\n",
      "step: 284500, gen_loss: 8.784853, disc_loss: 0.108970\n",
      "step: 285000, gen_loss: 7.399278, disc_loss: 0.033516\n",
      "step: 285500, gen_loss: 10.240234, disc_loss: 0.005380\n",
      "step: 286000, gen_loss: 8.832521, disc_loss: 0.052357\n",
      "step: 286500, gen_loss: 13.129766, disc_loss: 0.040988\n",
      "step: 287000, gen_loss: 9.526932, disc_loss: 0.005432\n",
      "step: 287500, gen_loss: 10.886086, disc_loss: 0.023026\n",
      "step: 288000, gen_loss: 10.839870, disc_loss: 0.004578\n",
      "step: 288500, gen_loss: 7.978541, disc_loss: 0.020597\n",
      "step: 289000, gen_loss: 11.742820, disc_loss: 0.061439\n",
      "step: 289500, gen_loss: 10.825973, disc_loss: 0.002044\n",
      "step: 290000, gen_loss: 10.232172, disc_loss: 0.028983\n",
      "step: 290500, gen_loss: 9.929783, disc_loss: 0.039838\n",
      "step: 291000, gen_loss: 7.902292, disc_loss: 0.013049\n",
      "step: 291500, gen_loss: 10.818499, disc_loss: 0.014663\n",
      "step: 292000, gen_loss: 9.872432, disc_loss: 0.030037\n",
      "step: 292500, gen_loss: 8.332344, disc_loss: 0.051212\n",
      "step: 293000, gen_loss: 7.546740, disc_loss: 0.030366\n",
      "step: 293500, gen_loss: 9.772099, disc_loss: 0.005819\n",
      "step: 294000, gen_loss: 9.907913, disc_loss: 0.004750\n",
      "step: 294500, gen_loss: 9.704926, disc_loss: 0.049298\n",
      "step: 295000, gen_loss: 11.781555, disc_loss: 0.067317\n",
      "step: 295500, gen_loss: 10.318466, disc_loss: 0.006763\n",
      "step: 296000, gen_loss: 12.213697, disc_loss: 0.008449\n",
      "step: 296500, gen_loss: 8.080772, disc_loss: 0.033005\n",
      "step: 297000, gen_loss: 10.617603, disc_loss: 0.012746\n",
      "step: 297500, gen_loss: 9.524639, disc_loss: 0.019866\n",
      "step: 298000, gen_loss: 10.637680, disc_loss: 0.001467\n",
      "step: 298500, gen_loss: 8.750874, disc_loss: 0.019551\n",
      "step: 299000, gen_loss: 6.238588, disc_loss: 0.039203\n",
      "step: 299500, gen_loss: 7.519877, disc_loss: 0.020380\n",
      "step: 300000, gen_loss: 10.174679, disc_loss: 0.013132\n",
      "step: 300500, gen_loss: 10.071970, disc_loss: 0.017378\n",
      "step: 301000, gen_loss: 10.001997, disc_loss: 0.013693\n",
      "step: 301500, gen_loss: 12.162846, disc_loss: 0.009850\n",
      "step: 302000, gen_loss: 11.015356, disc_loss: 0.019157\n",
      "step: 302500, gen_loss: 7.932765, disc_loss: 0.007933\n",
      "step: 303000, gen_loss: 7.959195, disc_loss: 0.017618\n",
      "step: 303500, gen_loss: 9.830533, disc_loss: 0.008865\n",
      "step: 304000, gen_loss: 11.529470, disc_loss: 0.102429\n",
      "step: 304500, gen_loss: 11.476850, disc_loss: 0.002422\n",
      "step: 305000, gen_loss: 9.940974, disc_loss: 0.001382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 305500, gen_loss: 12.992031, disc_loss: 0.010028\n",
      "step: 306000, gen_loss: 10.612424, disc_loss: 0.027950\n",
      "step: 306500, gen_loss: 9.458338, disc_loss: 0.010845\n",
      "step: 307000, gen_loss: 8.706457, disc_loss: 0.049829\n",
      "step: 307500, gen_loss: 7.134084, disc_loss: 0.027065\n",
      "step: 308000, gen_loss: 11.486773, disc_loss: 0.004552\n",
      "step: 308500, gen_loss: 8.831382, disc_loss: 0.029291\n",
      "step: 309000, gen_loss: 10.727647, disc_loss: 0.010856\n",
      "step: 309500, gen_loss: 12.099354, disc_loss: 0.027238\n",
      "step: 310000, gen_loss: 12.805078, disc_loss: 0.033556\n",
      "step: 310500, gen_loss: 10.750917, disc_loss: 0.010793\n",
      "step: 311000, gen_loss: 11.813082, disc_loss: 0.088587\n",
      "step: 311500, gen_loss: 10.729970, disc_loss: 0.003017\n",
      "step: 312000, gen_loss: 10.899867, disc_loss: 0.011729\n",
      "step: 312500, gen_loss: 8.386445, disc_loss: 0.005928\n",
      "step: 313000, gen_loss: 6.356719, disc_loss: 0.027332\n",
      "step: 313500, gen_loss: 11.600518, disc_loss: 0.004407\n",
      "step: 314000, gen_loss: 11.756420, disc_loss: 0.002950\n",
      "step: 314500, gen_loss: 8.736904, disc_loss: 0.005993\n",
      "step: 315000, gen_loss: 8.442184, disc_loss: 0.016429\n",
      "step: 315500, gen_loss: 7.778540, disc_loss: 0.010818\n",
      "step: 316000, gen_loss: 11.210613, disc_loss: 0.011293\n",
      "step: 316500, gen_loss: 12.936969, disc_loss: 0.015295\n",
      "step: 317000, gen_loss: 10.619635, disc_loss: 0.059774\n",
      "step: 317500, gen_loss: 12.249847, disc_loss: 0.058287\n",
      "step: 318000, gen_loss: 8.661091, disc_loss: 0.003944\n",
      "step: 318500, gen_loss: 7.760560, disc_loss: 0.005390\n",
      "step: 319000, gen_loss: 11.496124, disc_loss: 0.000809\n",
      "step: 319500, gen_loss: 8.971232, disc_loss: 0.009314\n",
      "step: 320000, gen_loss: 10.876919, disc_loss: 0.006396\n",
      "step: 320500, gen_loss: 10.050352, disc_loss: 0.005419\n",
      "step: 321000, gen_loss: 6.894821, disc_loss: 0.028804\n",
      "step: 321500, gen_loss: 7.896583, disc_loss: 0.013254\n",
      "step: 322000, gen_loss: 8.825346, disc_loss: 0.029922\n",
      "step: 322500, gen_loss: 11.804783, disc_loss: 0.018451\n",
      "step: 323000, gen_loss: 11.292063, disc_loss: 0.031953\n",
      "step: 323500, gen_loss: 10.794099, disc_loss: 0.007831\n",
      "step: 324000, gen_loss: 11.056700, disc_loss: 0.009072\n",
      "step: 324500, gen_loss: 9.356983, disc_loss: 0.076244\n",
      "step: 325000, gen_loss: 12.429169, disc_loss: 0.012297\n",
      "step: 325500, gen_loss: 10.904293, disc_loss: 0.004331\n",
      "step: 326000, gen_loss: 7.317445, disc_loss: 0.020941\n",
      "step: 326500, gen_loss: 7.796694, disc_loss: 0.006397\n",
      "step: 327000, gen_loss: 8.819330, disc_loss: 0.028082\n",
      "step: 327500, gen_loss: 11.099003, disc_loss: 0.003920\n",
      "step: 328000, gen_loss: 11.066553, disc_loss: 0.021809\n",
      "step: 328500, gen_loss: 11.177777, disc_loss: 0.011721\n",
      "step: 329000, gen_loss: 11.687306, disc_loss: 0.016193\n",
      "step: 329500, gen_loss: 11.531960, disc_loss: 0.005658\n",
      "step: 330000, gen_loss: 11.149302, disc_loss: 0.130344\n",
      "step: 330500, gen_loss: 10.394712, disc_loss: 0.004954\n",
      "step: 331000, gen_loss: 10.059791, disc_loss: 0.028954\n",
      "step: 331500, gen_loss: 7.835787, disc_loss: 0.114548\n",
      "step: 332000, gen_loss: 10.379780, disc_loss: 0.019274\n",
      "step: 332500, gen_loss: 9.058582, disc_loss: 0.033262\n",
      "step: 333000, gen_loss: 9.230646, disc_loss: 0.005326\n",
      "step: 333500, gen_loss: 8.700923, disc_loss: 0.012478\n",
      "step: 334000, gen_loss: 12.012596, disc_loss: 0.010543\n",
      "step: 334500, gen_loss: 10.362936, disc_loss: 0.058717\n",
      "step: 335000, gen_loss: 9.523180, disc_loss: 0.003455\n",
      "step: 335500, gen_loss: 12.227335, disc_loss: 0.011457\n",
      "step: 336000, gen_loss: 10.661510, disc_loss: 0.028181\n",
      "step: 336500, gen_loss: 12.441076, disc_loss: 0.002764\n",
      "step: 337000, gen_loss: 7.869310, disc_loss: 0.097336\n",
      "step: 337500, gen_loss: 8.801598, disc_loss: 0.019895\n",
      "step: 338000, gen_loss: 6.481905, disc_loss: 0.070995\n",
      "step: 338500, gen_loss: 7.322829, disc_loss: 0.097265\n",
      "step: 339000, gen_loss: 6.999369, disc_loss: 0.024563\n",
      "step: 339500, gen_loss: 9.188225, disc_loss: 0.003926\n",
      "step: 340000, gen_loss: 10.063515, disc_loss: 0.009760\n",
      "step: 340500, gen_loss: 9.128047, disc_loss: 0.011059\n",
      "step: 341000, gen_loss: 11.294241, disc_loss: 0.036177\n",
      "step: 341500, gen_loss: 9.240276, disc_loss: 0.003152\n",
      "step: 342000, gen_loss: 12.147341, disc_loss: 0.031336\n",
      "step: 342500, gen_loss: 11.240776, disc_loss: 0.004961\n",
      "step: 343000, gen_loss: 9.591393, disc_loss: 0.010670\n",
      "step: 343500, gen_loss: 10.261420, disc_loss: 0.003064\n",
      "step: 344000, gen_loss: 9.936306, disc_loss: 0.061409\n",
      "step: 344500, gen_loss: 9.645765, disc_loss: 0.023961\n",
      "step: 345000, gen_loss: 10.748842, disc_loss: 0.003907\n",
      "step: 345500, gen_loss: 10.009832, disc_loss: 0.020592\n",
      "step: 346000, gen_loss: 11.496509, disc_loss: 0.002922\n",
      "step: 346500, gen_loss: 10.596605, disc_loss: 0.015751\n",
      "step: 347000, gen_loss: 9.678955, disc_loss: 0.003403\n",
      "step: 347500, gen_loss: 9.265342, disc_loss: 0.032948\n",
      "step: 348000, gen_loss: 9.501423, disc_loss: 0.066012\n",
      "step: 348500, gen_loss: 9.045033, disc_loss: 0.010123\n",
      "step: 349000, gen_loss: 8.557714, disc_loss: 0.007699\n",
      "step: 349500, gen_loss: 9.656467, disc_loss: 0.010649\n",
      "step: 350000, gen_loss: 9.553312, disc_loss: 0.002547\n",
      "step: 350500, gen_loss: 13.790825, disc_loss: 0.004857\n",
      "step: 351000, gen_loss: 10.554541, disc_loss: 0.003506\n",
      "step: 351500, gen_loss: 9.126884, disc_loss: 0.031063\n",
      "step: 352000, gen_loss: 7.656279, disc_loss: 0.060651\n",
      "step: 352500, gen_loss: 9.256554, disc_loss: 0.022436\n",
      "step: 353000, gen_loss: 7.922993, disc_loss: 0.006236\n",
      "step: 353500, gen_loss: 13.694249, disc_loss: 0.097806\n",
      "step: 354000, gen_loss: 11.971299, disc_loss: 0.001339\n",
      "step: 354500, gen_loss: 11.567337, disc_loss: 0.004765\n",
      "step: 355000, gen_loss: 10.961633, disc_loss: 0.000909\n",
      "step: 355500, gen_loss: 10.069147, disc_loss: 0.038054\n",
      "step: 356000, gen_loss: 5.679513, disc_loss: 0.174769\n",
      "step: 356500, gen_loss: 12.736656, disc_loss: 0.004164\n",
      "step: 357000, gen_loss: 9.585350, disc_loss: 0.006964\n",
      "step: 357500, gen_loss: 10.182334, disc_loss: 0.000798\n",
      "step: 358000, gen_loss: 11.089173, disc_loss: 0.109942\n",
      "step: 358500, gen_loss: 10.200215, disc_loss: 0.102875\n",
      "step: 359000, gen_loss: 13.666702, disc_loss: 0.013499\n",
      "step: 359500, gen_loss: 6.533998, disc_loss: 0.061568\n",
      "step: 360000, gen_loss: 15.498138, disc_loss: 0.019906\n",
      "step: 360500, gen_loss: 10.291414, disc_loss: 0.072539\n",
      "step: 361000, gen_loss: 12.378510, disc_loss: 0.004568\n",
      "step: 361500, gen_loss: 10.789648, disc_loss: 0.155912\n",
      "step: 362000, gen_loss: 10.985879, disc_loss: 0.012566\n",
      "step: 362500, gen_loss: 11.784140, disc_loss: 0.007631\n",
      "step: 363000, gen_loss: 13.753643, disc_loss: 0.006133\n",
      "step: 363500, gen_loss: 9.223673, disc_loss: 0.005002\n",
      "step: 364000, gen_loss: 7.434032, disc_loss: 0.005126\n",
      "step: 364500, gen_loss: 10.848816, disc_loss: 0.010085\n",
      "step: 365000, gen_loss: 9.028809, disc_loss: 0.011359\n",
      "step: 365500, gen_loss: 10.530437, disc_loss: 0.020228\n",
      "step: 366000, gen_loss: 14.070851, disc_loss: 0.050267\n",
      "step: 366500, gen_loss: 10.233809, disc_loss: 0.007972\n",
      "step: 367000, gen_loss: 7.766402, disc_loss: 0.042092\n",
      "step: 367500, gen_loss: 11.041162, disc_loss: 0.009545\n",
      "step: 368000, gen_loss: 10.458422, disc_loss: 0.243261\n",
      "step: 368500, gen_loss: 8.656755, disc_loss: 0.003181\n",
      "step: 369000, gen_loss: 10.317320, disc_loss: 0.047194\n",
      "step: 369500, gen_loss: 9.224797, disc_loss: 0.020876\n",
      "step: 370000, gen_loss: 9.130787, disc_loss: 0.005772\n",
      "step: 370500, gen_loss: 10.425896, disc_loss: 0.020012\n",
      "step: 371000, gen_loss: 7.667232, disc_loss: 0.059417\n",
      "step: 371500, gen_loss: 9.499963, disc_loss: 0.003617\n",
      "step: 372000, gen_loss: 9.193407, disc_loss: 0.003171\n",
      "step: 372500, gen_loss: 9.862683, disc_loss: 0.008623\n",
      "step: 373000, gen_loss: 10.474164, disc_loss: 0.058955\n",
      "step: 373500, gen_loss: 10.166258, disc_loss: 0.006528\n",
      "step: 374000, gen_loss: 10.146313, disc_loss: 0.023107\n",
      "step: 374500, gen_loss: 14.042892, disc_loss: 0.011668\n",
      "step: 375000, gen_loss: 13.058867, disc_loss: 0.035930\n",
      "step: 375500, gen_loss: 13.426727, disc_loss: 0.021984\n",
      "step: 376000, gen_loss: 10.069042, disc_loss: 0.009901\n",
      "step: 376500, gen_loss: 10.395396, disc_loss: 0.000981\n",
      "step: 377000, gen_loss: 10.862715, disc_loss: 0.004262\n",
      "step: 377500, gen_loss: 10.002605, disc_loss: 0.002733\n",
      "step: 378000, gen_loss: 10.437963, disc_loss: 0.006019\n",
      "step: 378500, gen_loss: 10.913157, disc_loss: 0.002385\n",
      "step: 379000, gen_loss: 9.212099, disc_loss: 0.008954\n",
      "step: 379500, gen_loss: 9.359238, disc_loss: 0.000810\n",
      "step: 380000, gen_loss: 10.575858, disc_loss: 0.004291\n",
      "step: 380500, gen_loss: 10.613380, disc_loss: 0.046898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 381000, gen_loss: 11.234486, disc_loss: 0.005125\n",
      "step: 381500, gen_loss: 11.247149, disc_loss: 0.000614\n",
      "step: 382000, gen_loss: 8.055119, disc_loss: 0.019110\n",
      "step: 382500, gen_loss: 8.871382, disc_loss: 0.015874\n",
      "step: 383000, gen_loss: 7.250963, disc_loss: 0.094335\n",
      "step: 383500, gen_loss: 11.371673, disc_loss: 0.008273\n",
      "step: 384000, gen_loss: 11.994719, disc_loss: 0.033718\n",
      "step: 384500, gen_loss: 8.578048, disc_loss: 0.002582\n",
      "step: 385000, gen_loss: 11.626816, disc_loss: 0.001185\n",
      "step: 385500, gen_loss: 10.251695, disc_loss: 0.117652\n",
      "step: 386000, gen_loss: 8.984728, disc_loss: 0.001986\n",
      "step: 386500, gen_loss: 10.284032, disc_loss: 0.004166\n",
      "step: 387000, gen_loss: 10.755326, disc_loss: 0.007447\n",
      "step: 387500, gen_loss: 11.528546, disc_loss: 0.003384\n",
      "step: 388000, gen_loss: 13.308454, disc_loss: 0.001072\n",
      "step: 388500, gen_loss: 7.955778, disc_loss: 0.030041\n",
      "step: 389000, gen_loss: 9.829324, disc_loss: 0.022464\n",
      "step: 389500, gen_loss: 10.777086, disc_loss: 0.001469\n",
      "step: 390000, gen_loss: 10.498823, disc_loss: 0.008737\n",
      "step: 390500, gen_loss: 11.985462, disc_loss: 0.001437\n",
      "step: 391000, gen_loss: 13.925186, disc_loss: 0.001264\n",
      "step: 391500, gen_loss: 11.486444, disc_loss: 0.002488\n",
      "step: 392000, gen_loss: 12.389730, disc_loss: 0.002431\n",
      "step: 392500, gen_loss: 9.955884, disc_loss: 0.028031\n",
      "step: 393000, gen_loss: 10.881256, disc_loss: 0.014232\n",
      "step: 393500, gen_loss: 11.206370, disc_loss: 0.002261\n",
      "step: 394000, gen_loss: 11.886553, disc_loss: 0.034200\n",
      "step: 394500, gen_loss: 12.370621, disc_loss: 0.022404\n",
      "step: 395000, gen_loss: 6.591120, disc_loss: 0.073294\n",
      "step: 395500, gen_loss: 10.471192, disc_loss: 0.011028\n",
      "step: 396000, gen_loss: 9.544562, disc_loss: 0.001277\n",
      "step: 396500, gen_loss: 7.916615, disc_loss: 0.083989\n",
      "step: 397000, gen_loss: 8.815796, disc_loss: 0.005337\n",
      "step: 397500, gen_loss: 8.849283, disc_loss: 0.019208\n",
      "step: 398000, gen_loss: 10.761804, disc_loss: 0.001891\n",
      "step: 398500, gen_loss: 7.971643, disc_loss: 0.005114\n",
      "step: 399000, gen_loss: 11.464605, disc_loss: 0.107049\n",
      "step: 399500, gen_loss: 7.613408, disc_loss: 0.002802\n",
      "step: 400000, gen_loss: 11.425081, disc_loss: 0.000965\n",
      "step: 400500, gen_loss: 10.287062, disc_loss: 0.015794\n",
      "step: 401000, gen_loss: 10.701894, disc_loss: 0.016793\n",
      "step: 401500, gen_loss: 11.559084, disc_loss: 0.051151\n",
      "step: 402000, gen_loss: 11.222052, disc_loss: 0.004436\n",
      "step: 402500, gen_loss: 10.862453, disc_loss: 0.001974\n",
      "step: 403000, gen_loss: 9.906023, disc_loss: 0.016731\n",
      "step: 403500, gen_loss: 8.539797, disc_loss: 0.017576\n",
      "step: 404000, gen_loss: 8.446359, disc_loss: 0.014728\n",
      "step: 404500, gen_loss: 9.458111, disc_loss: 0.053215\n",
      "step: 405000, gen_loss: 11.176818, disc_loss: 0.002526\n",
      "step: 405500, gen_loss: 9.730255, disc_loss: 0.016480\n",
      "step: 406000, gen_loss: 9.952827, disc_loss: 0.008737\n",
      "step: 406500, gen_loss: 11.615765, disc_loss: 0.002762\n",
      "step: 407000, gen_loss: 11.247317, disc_loss: 0.001274\n",
      "step: 407500, gen_loss: 10.051767, disc_loss: 0.017192\n",
      "step: 408000, gen_loss: 12.206708, disc_loss: 0.039708\n",
      "step: 408500, gen_loss: 11.740620, disc_loss: 0.016830\n",
      "step: 409000, gen_loss: 9.109865, disc_loss: 0.003310\n",
      "step: 409500, gen_loss: 10.649296, disc_loss: 0.001502\n",
      "step: 410000, gen_loss: 11.894057, disc_loss: 0.002100\n",
      "step: 410500, gen_loss: 10.050306, disc_loss: 0.004213\n",
      "step: 411000, gen_loss: 9.256371, disc_loss: 0.014319\n",
      "step: 411500, gen_loss: 12.973224, disc_loss: 0.001453\n",
      "step: 412000, gen_loss: 9.198513, disc_loss: 0.006743\n",
      "step: 412500, gen_loss: 9.176493, disc_loss: 0.014848\n",
      "step: 413000, gen_loss: 12.527771, disc_loss: 0.033442\n",
      "step: 413500, gen_loss: 10.655144, disc_loss: 0.001001\n",
      "step: 414000, gen_loss: 9.895151, disc_loss: 0.002983\n",
      "step: 414500, gen_loss: 9.064002, disc_loss: 0.007217\n",
      "step: 415000, gen_loss: 12.355459, disc_loss: 0.002820\n",
      "step: 415500, gen_loss: 10.260399, disc_loss: 0.006627\n",
      "step: 416000, gen_loss: 10.453953, disc_loss: 0.016636\n",
      "step: 416500, gen_loss: 11.714426, disc_loss: 0.005528\n",
      "step: 417000, gen_loss: 10.497882, disc_loss: 0.002402\n",
      "step: 417500, gen_loss: 11.359554, disc_loss: 0.009460\n",
      "step: 418000, gen_loss: 10.644266, disc_loss: 0.001674\n",
      "step: 418500, gen_loss: 9.654436, disc_loss: 0.001612\n",
      "step: 419000, gen_loss: 12.655691, disc_loss: 0.011385\n",
      "step: 419500, gen_loss: 10.000386, disc_loss: 0.012754\n",
      "step: 420000, gen_loss: 12.315664, disc_loss: 0.020038\n",
      "step: 420500, gen_loss: 10.828338, disc_loss: 0.021123\n",
      "step: 421000, gen_loss: 10.068254, disc_loss: 0.012760\n",
      "step: 421500, gen_loss: 9.823316, disc_loss: 0.070965\n",
      "step: 422000, gen_loss: 11.570860, disc_loss: 0.014667\n",
      "step: 422500, gen_loss: 9.855466, disc_loss: 0.009163\n",
      "step: 423000, gen_loss: 8.809715, disc_loss: 0.029657\n",
      "step: 423500, gen_loss: 11.081590, disc_loss: 0.001572\n",
      "step: 424000, gen_loss: 11.141732, disc_loss: 0.008224\n",
      "step: 424500, gen_loss: 11.520170, disc_loss: 0.004190\n",
      "step: 425000, gen_loss: 10.179353, disc_loss: 0.028631\n",
      "step: 425500, gen_loss: 9.802274, disc_loss: 0.017654\n",
      "step: 426000, gen_loss: 11.516722, disc_loss: 0.002476\n",
      "step: 426500, gen_loss: 10.444814, disc_loss: 0.012981\n",
      "step: 427000, gen_loss: 11.145572, disc_loss: 0.011698\n",
      "step: 427500, gen_loss: 10.268647, disc_loss: 0.002240\n",
      "step: 428000, gen_loss: 9.296619, disc_loss: 0.042088\n",
      "step: 428500, gen_loss: 11.854937, disc_loss: 0.004467\n",
      "step: 429000, gen_loss: 12.613398, disc_loss: 0.032051\n",
      "step: 429500, gen_loss: 5.571966, disc_loss: 0.303539\n",
      "step: 430000, gen_loss: 9.944485, disc_loss: 0.007772\n",
      "step: 430500, gen_loss: 13.496336, disc_loss: 0.002126\n",
      "step: 431000, gen_loss: 12.168976, disc_loss: 0.014414\n",
      "step: 431500, gen_loss: 11.126881, disc_loss: 0.001764\n",
      "step: 432000, gen_loss: 9.101261, disc_loss: 0.033561\n",
      "step: 432500, gen_loss: 10.438131, disc_loss: 0.033497\n",
      "step: 433000, gen_loss: 10.989982, disc_loss: 0.006997\n",
      "step: 433500, gen_loss: 9.840854, disc_loss: 0.048672\n",
      "step: 434000, gen_loss: 11.352372, disc_loss: 0.025227\n",
      "step: 434500, gen_loss: 12.578148, disc_loss: 0.010700\n",
      "step: 435000, gen_loss: 12.417222, disc_loss: 0.026582\n",
      "step: 435500, gen_loss: 12.395063, disc_loss: 0.003651\n",
      "step: 436000, gen_loss: 11.646660, disc_loss: 0.003164\n",
      "step: 436500, gen_loss: 9.953888, disc_loss: 0.016752\n",
      "step: 437000, gen_loss: 12.702149, disc_loss: 0.001893\n",
      "step: 437500, gen_loss: 11.428947, disc_loss: 0.010259\n",
      "step: 438000, gen_loss: 11.558878, disc_loss: 0.000603\n",
      "step: 438500, gen_loss: 10.292282, disc_loss: 0.002864\n",
      "step: 439000, gen_loss: 14.494736, disc_loss: 0.001160\n",
      "step: 439500, gen_loss: 10.189570, disc_loss: 0.001282\n",
      "step: 440000, gen_loss: 15.446795, disc_loss: 0.312777\n",
      "step: 440500, gen_loss: 10.599875, disc_loss: 0.006776\n",
      "step: 441000, gen_loss: 11.802795, disc_loss: 0.002847\n",
      "step: 441500, gen_loss: 9.912384, disc_loss: 0.007872\n",
      "step: 442000, gen_loss: 11.470384, disc_loss: 0.016411\n",
      "step: 442500, gen_loss: 9.989156, disc_loss: 0.006770\n",
      "step: 443000, gen_loss: 12.966437, disc_loss: 0.134504\n",
      "step: 443500, gen_loss: 8.285765, disc_loss: 0.032413\n",
      "step: 444000, gen_loss: 10.681495, disc_loss: 0.011622\n",
      "step: 444500, gen_loss: 11.949186, disc_loss: 0.001728\n",
      "step: 445000, gen_loss: 14.239276, disc_loss: 0.026017\n",
      "step: 445500, gen_loss: 9.673656, disc_loss: 0.030451\n",
      "step: 446000, gen_loss: 9.702777, disc_loss: 0.008929\n",
      "step: 446500, gen_loss: 12.398283, disc_loss: 0.003597\n",
      "step: 447000, gen_loss: 12.033800, disc_loss: 0.009731\n",
      "step: 447500, gen_loss: 11.174494, disc_loss: 0.030321\n",
      "step: 448000, gen_loss: 11.496080, disc_loss: 0.004634\n",
      "step: 448500, gen_loss: 10.486610, disc_loss: 0.030101\n",
      "step: 449000, gen_loss: 10.855217, disc_loss: 0.002903\n",
      "step: 449500, gen_loss: 10.736661, disc_loss: 0.021269\n",
      "step: 450000, gen_loss: 12.463251, disc_loss: 0.000662\n",
      "step: 450500, gen_loss: 9.721316, disc_loss: 0.005431\n",
      "step: 451000, gen_loss: 12.019629, disc_loss: 0.005845\n",
      "step: 451500, gen_loss: 13.872131, disc_loss: 0.000657\n",
      "step: 452000, gen_loss: 12.421701, disc_loss: 0.005896\n",
      "step: 452500, gen_loss: 11.780138, disc_loss: 0.020769\n",
      "step: 453000, gen_loss: 10.100052, disc_loss: 0.022009\n",
      "step: 453500, gen_loss: 9.272209, disc_loss: 0.007860\n",
      "step: 454000, gen_loss: 7.161274, disc_loss: 0.079980\n",
      "step: 454500, gen_loss: 9.843313, disc_loss: 0.031642\n",
      "step: 455000, gen_loss: 15.204958, disc_loss: 0.054638\n",
      "step: 455500, gen_loss: 13.847743, disc_loss: 0.049224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 456000, gen_loss: 8.961699, disc_loss: 0.006878\n",
      "step: 456500, gen_loss: 13.866675, disc_loss: 0.007559\n",
      "step: 457000, gen_loss: 11.232466, disc_loss: 0.081618\n",
      "step: 457500, gen_loss: 6.917402, disc_loss: 0.009118\n",
      "step: 458000, gen_loss: 11.246431, disc_loss: 0.002788\n",
      "step: 458500, gen_loss: 12.236203, disc_loss: 0.002139\n",
      "step: 459000, gen_loss: 10.519258, disc_loss: 0.006486\n",
      "step: 459500, gen_loss: 8.878487, disc_loss: 0.001364\n",
      "step: 460000, gen_loss: 9.807013, disc_loss: 0.028853\n",
      "step: 460500, gen_loss: 8.740797, disc_loss: 0.031604\n",
      "step: 461000, gen_loss: 12.172924, disc_loss: 0.000725\n",
      "step: 461500, gen_loss: 9.376235, disc_loss: 0.081470\n",
      "step: 462000, gen_loss: 9.548243, disc_loss: 0.007036\n",
      "step: 462500, gen_loss: 12.037374, disc_loss: 0.001945\n",
      "step: 463000, gen_loss: 12.989830, disc_loss: 0.024992\n",
      "step: 463500, gen_loss: 7.405709, disc_loss: 0.030590\n",
      "step: 464000, gen_loss: 9.445966, disc_loss: 0.015966\n",
      "step: 464500, gen_loss: 9.651678, disc_loss: 0.003231\n",
      "step: 465000, gen_loss: 8.982059, disc_loss: 0.004585\n",
      "step: 465500, gen_loss: 11.898361, disc_loss: 0.008971\n",
      "step: 466000, gen_loss: 12.301863, disc_loss: 0.064018\n",
      "step: 466500, gen_loss: 9.585719, disc_loss: 0.001120\n",
      "step: 467000, gen_loss: 11.550823, disc_loss: 0.002222\n",
      "step: 467500, gen_loss: 10.943874, disc_loss: 0.003733\n",
      "step: 468000, gen_loss: 11.894676, disc_loss: 0.011288\n",
      "step: 468500, gen_loss: 13.013709, disc_loss: 0.000462\n",
      "step: 469000, gen_loss: 10.577852, disc_loss: 0.004636\n",
      "step: 469500, gen_loss: 10.588477, disc_loss: 0.006112\n",
      "step: 470000, gen_loss: 10.032421, disc_loss: 0.025872\n",
      "step: 470500, gen_loss: 7.938862, disc_loss: 0.006641\n",
      "step: 471000, gen_loss: 10.513086, disc_loss: 0.001521\n",
      "step: 471500, gen_loss: 9.362181, disc_loss: 0.025534\n",
      "step: 472000, gen_loss: 9.389572, disc_loss: 0.011614\n",
      "step: 472500, gen_loss: 11.836603, disc_loss: 0.007546\n",
      "step: 473000, gen_loss: 12.476743, disc_loss: 0.043860\n",
      "step: 473500, gen_loss: 11.874255, disc_loss: 0.008271\n",
      "step: 474000, gen_loss: 12.536575, disc_loss: 0.011237\n",
      "step: 474500, gen_loss: 11.596029, disc_loss: 0.005010\n",
      "step: 475000, gen_loss: 12.512922, disc_loss: 0.313093\n",
      "step: 475500, gen_loss: 15.040517, disc_loss: 0.022150\n",
      "step: 476000, gen_loss: 11.076111, disc_loss: 0.008388\n",
      "step: 476500, gen_loss: 9.892115, disc_loss: 0.012845\n",
      "step: 477000, gen_loss: 11.770555, disc_loss: 0.009252\n",
      "step: 477500, gen_loss: 10.986816, disc_loss: 0.010921\n",
      "step: 478000, gen_loss: 8.813890, disc_loss: 0.014114\n",
      "step: 478500, gen_loss: 14.639273, disc_loss: 0.002185\n",
      "step: 479000, gen_loss: 9.180862, disc_loss: 0.000813\n",
      "step: 479500, gen_loss: 7.998732, disc_loss: 0.109552\n",
      "step: 480000, gen_loss: 9.484530, disc_loss: 0.009237\n",
      "step: 480500, gen_loss: 9.778912, disc_loss: 0.001354\n",
      "step: 481000, gen_loss: 11.530569, disc_loss: 0.026077\n",
      "step: 481500, gen_loss: 11.131325, disc_loss: 0.015994\n",
      "step: 482000, gen_loss: 9.863487, disc_loss: 0.000784\n",
      "step: 482500, gen_loss: 12.763483, disc_loss: 0.066163\n",
      "step: 483000, gen_loss: 12.981905, disc_loss: 0.020057\n",
      "step: 483500, gen_loss: 9.592524, disc_loss: 0.002152\n",
      "step: 484000, gen_loss: 11.068974, disc_loss: 0.006066\n",
      "step: 484500, gen_loss: 12.762001, disc_loss: 0.001734\n",
      "step: 485000, gen_loss: 11.986682, disc_loss: 0.116981\n",
      "step: 485500, gen_loss: 11.903688, disc_loss: 0.002421\n",
      "step: 486000, gen_loss: 12.131798, disc_loss: 0.000909\n",
      "step: 486500, gen_loss: 13.883211, disc_loss: 0.023662\n",
      "step: 487000, gen_loss: 11.644022, disc_loss: 0.064828\n",
      "step: 487500, gen_loss: 10.243078, disc_loss: 0.002540\n",
      "step: 488000, gen_loss: 10.779995, disc_loss: 0.000345\n",
      "step: 488500, gen_loss: 12.303492, disc_loss: 0.008991\n",
      "step: 489000, gen_loss: 12.961280, disc_loss: 0.051159\n",
      "step: 489500, gen_loss: 12.626978, disc_loss: 0.003103\n",
      "step: 490000, gen_loss: 10.277045, disc_loss: 0.003855\n",
      "step: 490500, gen_loss: 13.038322, disc_loss: 0.027738\n",
      "step: 491000, gen_loss: 10.593132, disc_loss: 0.001207\n",
      "step: 491500, gen_loss: 11.349575, disc_loss: 0.007979\n",
      "step: 492000, gen_loss: 12.066637, disc_loss: 0.001054\n",
      "step: 492500, gen_loss: 10.863764, disc_loss: 0.001624\n",
      "step: 493000, gen_loss: 11.010550, disc_loss: 0.006330\n",
      "step: 493500, gen_loss: 12.049204, disc_loss: 0.000807\n",
      "step: 494000, gen_loss: 12.594147, disc_loss: 0.004407\n",
      "step: 494500, gen_loss: 10.388514, disc_loss: 0.093395\n",
      "step: 495000, gen_loss: 10.298851, disc_loss: 0.010695\n",
      "step: 495500, gen_loss: 10.521379, disc_loss: 0.009076\n",
      "step: 496000, gen_loss: 11.751801, disc_loss: 0.017540\n",
      "step: 496500, gen_loss: 11.074060, disc_loss: 0.013028\n",
      "step: 497000, gen_loss: 12.660259, disc_loss: 0.013744\n",
      "step: 497500, gen_loss: 10.794783, disc_loss: 0.006363\n",
      "step: 498000, gen_loss: 11.726515, disc_loss: 0.003206\n",
      "step: 498500, gen_loss: 13.254135, disc_loss: 0.002730\n",
      "step: 499000, gen_loss: 10.744811, disc_loss: 0.016479\n",
      "step: 499500, gen_loss: 12.249360, disc_loss: 0.020518\n",
      "step: 500000, gen_loss: 9.971767, disc_loss: 0.001687\n",
      "step: 500500, gen_loss: 9.975016, disc_loss: 0.004572\n",
      "step: 501000, gen_loss: 13.010371, disc_loss: 0.001224\n",
      "step: 501500, gen_loss: 11.255945, disc_loss: 0.002293\n",
      "step: 502000, gen_loss: 10.308026, disc_loss: 0.038849\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-00b75e7f4adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Run the optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-16e259888ab7>\u001b[0m in \u001b[0;36mrun_optimization\u001b[0;34m(real_images, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgradients_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_ones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m   \u001b[0mas_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps + 1)):\n",
    "    batch_y = to_categorical(batch_y)\n",
    "    batch_y = tf.cast(batch_y, tf.float32)\n",
    "    \n",
    "    if step == 0:\n",
    "        # Generate noise.\n",
    "        noise = np.random.normal(-1., 1., size=[batch_size, noise_dim]).astype(np.float32)\n",
    "        gen_loss = generator_loss(discriminator(generator(noise, batch_y), batch_y))\n",
    "        disc_loss = discriminator_loss(discriminator(batch_x, batch_y), discriminator(generator(noise, batch_y), batch_y))\n",
    "        print(\"initial: gen_loss: %f, disc_loss: %f\" % (gen_loss, disc_loss))\n",
    "        continue\n",
    "    \n",
    "    # Run the optimization.\n",
    "    gen_loss, disc_loss = run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        print(\"step: %i, gen_loss: %f, disc_loss: %f\" % (step, gen_loss, disc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "# Generate images from noise, using the generator network.\n",
    "write_number = 9\n",
    "\n",
    "n = 6\n",
    "canvas = np.empty((28 * n, 28 * n))\n",
    "for i in range(n):\n",
    "    # Noise input.\n",
    "    z = np.random.normal(-1., 1., size=[n, noise_dim]).astype(np.float32)\n",
    "    label = np.zeros([6,10])\n",
    "    label[:,write_number] = 1.0\n",
    "    # Generate image from noise.\n",
    "    g = generator(z,label).numpy()\n",
    "    # Rescale to original [0, 1]\n",
    "    g = (g + 1.) / 2\n",
    "    # Reverse colours for better display\n",
    "    g = -1 * (g - 1)\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits\n",
    "        canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
    "\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

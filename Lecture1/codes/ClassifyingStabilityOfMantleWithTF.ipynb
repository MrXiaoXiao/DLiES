{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Stability of Mantle with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stability of Mantle is defined by Rayleigh number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When using Jupyter notebook make sure to call tf.reset_default_graph() \n",
    "# at the beginning to clear the symbolic graph before defining new nodes.\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to generate data set\n",
    "def generate_data_set(instance_num = 10000, split_rate = 0.6):\n",
    "    #instance[0]  Gravitational acceleration\n",
    "    #instance[1]  Volume expansion coefficient\n",
    "    #instance[2]  Kinematic viscosity coefficient \n",
    "    #instance[3]  Thermal diffusivity\n",
    "    #instance[4]  Depth\n",
    "    #instance[5]  b\n",
    "    #instance[6] λ\n",
    "    #instance[7] (T0 - T1)/1000\n",
    "    \n",
    "    #label (1,0) for stable (0,1) for unstable\n",
    "    \n",
    "    #instance[8] stability 0 is unstable and 1 is stable\n",
    "    counter_for_stable = 0\n",
    "    counter_for_unstable = 0\n",
    "    \n",
    "    data_set = {'input':np.zeros([instance_num, 8]),'label':np.zeros([instance_num,2])}\n",
    "    \n",
    "    #simulate gravitational accelerations\n",
    "    data_set['input'][:,0] = np.random.uniform(0.8,1.0,size=instance_num)\n",
    "    \n",
    "    #simulate Volume expansion coefficient\n",
    "    data_set['input'][:,1] = np.random.uniform(1e-4,1e-2,size=instance_num)\n",
    "    \n",
    "    #simulate Kinematic viscosity coefficient\n",
    "    data_set['input'][:,2] = np.random.uniform(1e-2,1.0,size=instance_num)\n",
    "    \n",
    "    #simulate Thermal diffusivity\n",
    "    data_set['input'][:,3] = np.random.uniform(0.1,1.0,size=instance_num)\n",
    "    \n",
    "    #simulate Depth 10000km\n",
    "    data_set['input'][:,4] = np.random.uniform(0,0.35,size=instance_num)\n",
    "    \n",
    "    #simulate b 10000km\n",
    "    for idx in range(instance_num):    \n",
    "        data_set['input'][idx,5] = np.random.uniform(max([0.25,data_set['input'][idx,4]]),0.35)\n",
    "    \n",
    "    #simulate λ\n",
    "    data_set['input'][:,6] = np.random.uniform(0.0,0.39,size=instance_num)\n",
    "    \n",
    "    #simulate T0 - T1\n",
    "    data_set['input'][:,7] = np.random.uniform(0.0,0.5,size=instance_num)\n",
    "    \n",
    "    for idx in range(instance_num):\n",
    "        #\n",
    "        g = data_set['input'][idx,0]*10\n",
    "        a = data_set['input'][idx,1]\n",
    "        v = data_set['input'][idx,2]*1000\n",
    "        k = data_set['input'][idx,3]*10000\n",
    "        d = data_set['input'][idx,4]*10000\n",
    "        b = data_set['input'][idx,5]*10000\n",
    "        lam = data_set['input'][idx,6]*10000\n",
    "        dT = data_set['input'][idx,7]*1000\n",
    "        \n",
    "        Ra = (a*g*dT*(d**3))/(v*k)\n",
    "        \n",
    "        Racr = (np.pi**4*((4+(lam/b)**2)**3))/(4*((lam/b)**4))\n",
    "        \n",
    "        if Ra > Racr:\n",
    "            data_set['label'][idx,0] = 0\n",
    "            data_set['label'][idx,1] = 1\n",
    "            counter_for_unstable += 1\n",
    "        else:\n",
    "            data_set['label'][idx,0] = 1\n",
    "            data_set['label'][idx,1] = 0\n",
    "            counter_for_stable += 1\n",
    "    split_index = int(instance_num*split_rate)\n",
    "    \n",
    "    train_set = {'input':data_set['input'][0:split_index,:],\n",
    "                 'label':data_set['label'][0:split_index,:]}\n",
    "    test_set = {'input':data_set['input'][split_index:,:],\n",
    "                 'label':data_set['label'][split_index:,:]}\n",
    "    \n",
    "    print('Stable:{} UnStable:{}'.format(counter_for_stable, counter_for_unstable))\n",
    "    \n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable:574480 UnStable:425520\n"
     ]
    }
   ],
   "source": [
    "data_set_size = 1000000\n",
    "split_rate = 0.5\n",
    "train_set, test_set = generate_data_set(instance_num = data_set_size, split_rate = split_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define full connection layer\n",
    "def full_connection_layer(input_tensor, n_out, \n",
    "                          w_init=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          b_init=tf.constant_initializer(0.1), \n",
    "                          activation=tf.nn.sigmoid, name=None):\n",
    "    n_in = input_tensor.get_shape().as_list()[1]\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable('weight',[n_in, n_out],initializer=w_init)\n",
    "        bias = tf.get_variable('bias',[n_out],initializer=b_init)\n",
    "    output_tensor = activation(tf.matmul(input_tensor,weight)+bias,name=name+'_output')\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor):\n",
    "    hidden_layer_1 = full_connection_layer(input_tensor=input_tensor,n_out=4, name='fc_layer_1')\n",
    "    hidden_layer_2 = full_connection_layer(input_tensor=hidden_layer_1,n_out=4, name='fc_layer_2')\n",
    "    hidden_layer_3 = full_connection_layer(input_tensor=hidden_layer_2,n_out=4, name='fc_layer_3')\n",
    "    pred = full_connection_layer(input_tensor=hidden_layer_3,n_out=2, name='pred')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set param for training\n",
    "step_num = 20001\n",
    "batch_size = 1000\n",
    "data_length = 8\n",
    "learning_rate = 0.01\n",
    "#setup training \n",
    "input_tensor = tf.placeholder(tf.float32,[None,data_length], name='input')\n",
    "label = tf.placeholder(tf.float32,[None,2], name='label')\n",
    "pred = inference(input_tensor=input_tensor)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=label))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traning_batch(train_set, batch_size, data_length):\n",
    "    batch_ids = np.random.choice(len(train_set['input']),batch_size)\n",
    "    input_batch = np.zeros([batch_size,data_length])\n",
    "    label_batch = np.zeros([batch_size,2])\n",
    "    for idx in range(batch_size):\n",
    "        input_batch[idx][:] = train_set['input'][batch_ids[idx]][:]\n",
    "        label_batch[idx][:] = train_set['label'][batch_ids[idx]][:]\n",
    "    return input_batch,label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934938\n",
      "0.39525732\n",
      "0.3629702\n",
      "0.3573489\n",
      "0.35604522\n",
      "0.34598345\n",
      "0.34709588\n",
      "0.34188947\n",
      "0.354488\n",
      "0.3468753\n",
      "0.3486143\n"
     ]
    }
   ],
   "source": [
    "#start traning\n",
    "for idx in range(step_num):\n",
    "    input_batch, label_batch = get_traning_batch(train_set, batch_size, data_length)\n",
    "    _, loss_val = sess.run([train_op, loss], {input_tensor: input_batch, label: label_batch})\n",
    "    if idx%2000 == 0:\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649959802627563\n"
     ]
    }
   ],
   "source": [
    "correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(label,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print('Accuracy: {}'.format(sess.run(accuracy,{input_tensor: test_set['input'], label: test_set['label']})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
